{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape(url,df_dict):  #to scrape 'Title','Company','Location','Salary' and 'JobSummary' from each HTML page\n",
    "    # establishing the connection to the webpage\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # You can use status codes to understand how the target server responds to your request.\n",
    "    #Ex. 200 = OK, 400 = Bad Request, 403 = Forbidden, 404 = Not Found\n",
    "    print('Status Code: ',response.status_code)\n",
    "\n",
    "    if response.status_code==200:\n",
    "        # Pull HTML string out of requests and convert to a python string\n",
    "        html = response.text\n",
    "        soup = BeautifulSoup(html, 'lxml')  #'parsing HTML data'\n",
    "        record=soup.findAll('article',{'data-automation':'normalJob'})\n",
    "        for i in range(len(record)): #iteration through records\n",
    "            s0=record[i].find('span',{'class':'lwHBT6d'}) #extract salary\n",
    "            if s0:\n",
    "\n",
    "                df_dict['Salary'].append(s0.text)\n",
    "            else:\n",
    "                df_dict['Salary'].append('NA')\n",
    "\n",
    "\n",
    "            t0=record[i]['aria-label'] #job title\n",
    "            if t0:\n",
    "                df_dict['Title'].append(t0)\n",
    "            else:\n",
    "                df_dict['Title'].append('NA')\n",
    "\n",
    "            c0=record[i].find('a',{'data-automation':\"jobCompany\"}) #company name\n",
    "            if c0:\n",
    "                df_dict['Company'].append(c0.text)\n",
    "            else:\n",
    "                df_dict['Company'].append('NA')\n",
    "\n",
    "            d0=record[i].find('span',{'class':\"bl7UwXp\"}).text #short description\n",
    "            if d0:\n",
    "                df_dict['ShortDesp'].append(d0)\n",
    "            else:\n",
    "                df_dict['ShortDesp'].append('NA')\n",
    "\n",
    "            l0=record[i].find('a',{'data-automation':\"jobTitle\"}) #link to details of full job description\n",
    "            if l0:\n",
    "                links='https://www.seek.com.au'+l0['href']\n",
    "                rep=requests.get(links)\n",
    "                #print(rep.status_code)\n",
    "                if rep.status_code==200:\n",
    "                    hsum=rep.text\n",
    "                    soup_link=BeautifulSoup(hsum, 'lxml')\n",
    "                    dsp=soup_link.find('div',{'data-automation':\"mobileTemplate\"})\n",
    "                    if dsp:\n",
    "                    #dsp.get_text()\n",
    "                        df_dict['JobSummary'].append(dsp.get_text())\n",
    "\n",
    "                        s1=soup_link.find('section',{'role':'region'})\n",
    "                        s2=s1.findAll('strong')\n",
    "                        df_dict['Location'].append(s2[0].text)\n",
    "                        df_dict['Industry'].append(s2[1].text)\n",
    "                    else:\n",
    "                        df_dict['JobSummary'].append('NA')\n",
    "                        df_dict['Location'].append('NA')\n",
    "                        df_dict['Industry'].append('NA')\n",
    "            \n",
    "    return response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def category_scrape(filename,lk,nst,ned):\n",
    "    index=0\n",
    "    page=1\n",
    "    html=[]\n",
    "    df_dict={'Title':[],'Company':[],'Location':[], 'Salary':[],'JobSummary':[],'Industry':[],'ShortDesp':[]}\n",
    "    for page in range(nst,ned+1):\n",
    "        weblink=lk+str(page)\n",
    "        print(weblink)\n",
    "        r=scrape(weblink,df_dict)\n",
    "        #print('return code:',r)\n",
    "        if r!=200:\n",
    "            break\n",
    "    df=pd.DataFrame(df_dict)\n",
    "    print(len(df))\n",
    "    filename+='.csv'\n",
    "    df.to_csv(filename, sep='\\t', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
